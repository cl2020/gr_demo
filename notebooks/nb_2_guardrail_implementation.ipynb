{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Guardrail Implementation  \n",
        "**Colin Li @ 2025-03**\n",
        "\n",
        "This notebook continues from `nb_1_guardrail_design.ipynb` and focuses on implementing the designed guardrails in a RAG-based chatbot using the open-source GenAI model `gemma-7b-it`. It demonstrates the **effectiveness** of the guardrails both **qualitatively** and **quantitatively**, and showcases alignment with **best practice** commonly adopted by cloud providers such as AWS.\n",
        "\n"
      ],
      "metadata": {
        "id": "sSuTJ55oLO7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README Before You Start\n",
        "\n",
        "- Colab Runtime Setup\n",
        "    - **Tested in**: Colab Pro only  \n",
        "    - **Runtime type**: Python 3  \n",
        "    - **Hardware accelerator**: T4 GPU  \n",
        "    - **High-RAM**: Enabled\n",
        "\n",
        "- Preparation\n",
        "    - Create a Hugging Face account and access token\n",
        "    - Log in and accept model terms for:\n",
        "        - [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
        "        - [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it)\n",
        "    - Follow the instruction below to:\n",
        "        - Install the required Python packages (listed below)\n",
        "        - Enter Hugging Face token (Note: you will have to enter it)\n",
        "        - Download knowledge base (PDF) from github repo\n",
        "\n",
        "\n",
        "- Note for Local Setup\n",
        "    - This notebook is GPU-intensive. Running locally without an **NVIDIA GPU** is not recommended.  \n",
        "    - If running locally, ensure at least **15GB of available VRAM**.\n"
      ],
      "metadata": {
        "id": "fFf47Uyrc363"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm\n",
        "!pip install torch\n",
        "!pip install gradio\n",
        "!pip install PyMuPDF\n",
        "!pip install faiss-cpu\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install transformers\n",
        "!pip install guardrails-ai\n",
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!pip install sentence-transformers\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "aP67CEKj-Uqu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required for Chatbot\n",
        "import gc\n",
        "import os\n",
        "import fitz\n",
        "import faiss\n",
        "import torch\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from getpass import getpass\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import output\n",
        "from huggingface_hub import login\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig)"
      ],
      "metadata": {
        "id": "qWTVjLOH3w_g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required for GRs\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "from guardrails import validator_base\n",
        "from typing import Any, Dict, List, Optional\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from guardrails.validator_base import (\n",
        "    Validator,\n",
        "    register_validator,\n",
        "    FailResult,\n",
        "    PassResult,\n",
        "    ValidationResult)"
      ],
      "metadata": {
        "id": "F8CimGOPcCZN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your Hugging Face access token here\n",
        "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
        "\n",
        "# Login to huggingface first\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUDy2TTSeiuZ",
        "outputId": "f793bd02-5e2b-4164-d6d2-32610b11738d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face access token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data from the github repo created for this demo\n",
        "!wget https://github.com/cl2020/gr_demo/raw/main/data/ultimate-awards.pdf -O ultimate-awards.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf-IWIRuharV",
        "outputId": "d587ad58-7920-477a-d9fa-e6bc9b75a1ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-28 03:13:57--  https://github.com/cl2020/gr_demo/raw/main/data/ultimate-awards.pdf\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/cl2020/gr_demo/main/data/ultimate-awards.pdf [following]\n",
            "--2025-03-28 03:13:58--  https://media.githubusercontent.com/media/cl2020/gr_demo/main/data/ultimate-awards.pdf\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3170124 (3.0M) [application/octet-stream]\n",
            "Saving to: ‘ultimate-awards.pdf’\n",
            "\n",
            "ultimate-awards.pdf 100%[===================>]   3.02M  9.07MB/s    in 0.3s    \n",
            "\n",
            "2025-03-28 03:14:00 (9.07 MB/s) - ‘ultimate-awards.pdf’ saved [3170124/3170124]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read pdf which is the knowledge base for RAG\n",
        "fn_pdf = 'ultimate-awards.pdf'\n",
        "if fn_pdf in os.listdir():\n",
        "    doc = fitz.open(fn_pdf)\n",
        "else:\n",
        "    raise IOError(\"Failed to download. Please manually update file to Colab.\")"
      ],
      "metadata": {
        "id": "XNY-Yxd_jSqB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Setup Guardrails\n",
        "- Refer to `nb_1_guardrail_design.ipynb` for detailed explanation\n",
        "- Make all all LLL in the next cell are loaded without errors"
      ],
      "metadata": {
        "id": "K1iJbfMO2UMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Hallucination Detection\n",
        "nli_model_1 = pipeline(\n",
        "    \"text-classification\", model='GuardrailsAI/finetuned_nli_provenance')\n",
        "\n",
        "# Topic Classification\n",
        "topic_model = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model='facebook/bart-large-mnli',\n",
        "    hypothesis_template=\"This sentence above contains discussions of the folllowing topics: {}.\",\n",
        "    multi_label=True)\n",
        "\n",
        "# English Langauge Detection\n",
        "lan_class_model = pipeline(\n",
        "    \"text-classification\", model=\"qanastek/51-languages-classifier\")\n",
        "\n",
        "# Toxic Language Detection\n",
        "toxicity_model = pipeline(\n",
        "    \"text-classification\", model=\"JungleLee/bert-toxic-comment-classification\")\n",
        "\n",
        "# PII Detection and Anonymisation\n",
        "logging.getLogger(\"presidio-analyzer\").setLevel(logging.ERROR)\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer= AnonymizerEngine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck6FkdQR2MgM",
        "outputId": "d4ec7e72-5414-42d4-e715-a24ddc64ff28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "pii_entities = [\n",
        "    \"PERSON\", \"FIRST_NAME\", \"LAST_NAME\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\",\n",
        "    \"LOCATION\", \"CITY\", \"STATE_OR_PROVINCE\", \"COUNTRY\", \"ZIP_CODE\",\n",
        "    \"CREDIT_CARD\", \"IN_PAN\"]\n",
        "\n",
        "allowed_topics = [\n",
        "    \"credit card\", \"ultimate awards\", \"annual fees\", \"credit limit\",\n",
        "    \"minimum repayment\", \"interest rates\", \"interest-free days\",\n",
        "    \"monthly fees\", \"cash advance fees\", \"late payment fees\",\n",
        "    \"awards points\", \"qantas points\", \"earning points\",\n",
        "    \"points offers\", \"cashback offers\", \"redeeming points\",\n",
        "    \"international transaction fee\", \"travel insurance\"]\n",
        "\n",
        "banned_topics = [\n",
        "    \"home loan\", \"mortgage\", \"personal loan\", \"savings account\",\n",
        "    \"term deposit\", \"transaction account\", \"offset account\", \"insurance\",\n",
        "    \"superannuation\", \"business account\", \"investments\",\n",
        "    \"financial advice\", \"investment advice\", \"complaint\", \"refund\",\n",
        "    \"scam\", \"fraud\", \"password\", \"update personal details\", \"speak to manager\",\n",
        "    \"account locked\", \"privacy\", \"data breach\", \"legal\", \"lawsuit\",\n",
        "    \"joke\", \"preference\", \"pizza\", \"weather\", \"news\", \"sports\", \"religion\",\n",
        "    \"politics\", \"ethics\", \"opinion\"]\n",
        "\n",
        "competitors_dict = {\n",
        "    \"ANZ\": \"ANZ\", \"Westpac\": \"Westpac\", \"NAB\": \"National Australia Bank\",\n",
        "    \"Macquarie\": \"Macquarie Bank\", \"Amex\": \"American Express\",\n",
        "    \"Afterpay\": \"Afterpay\"}\n",
        "\n",
        "# Helper function\n",
        "def get_result(result, verbose=True):\n",
        "    \"\"\"\n",
        "    Extracts validation result details. Converts the result object into a\n",
        "    dictionary with relevant fields depending on the outcome.\n",
        "    - If the outcome is 'pass', it loads and includes the value override.\n",
        "    - If the outcome is 'fail', it loads the error message and adds the fix\n",
        "      response.\n",
        "\n",
        "    Args:\n",
        "        result: A ValidationResult object with outcome, value_override,\n",
        "                error_message, and fix_value.\n",
        "        verbose (bool): Whether to print result details to the console.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the outcome and relevant details.\n",
        "    \"\"\"\n",
        "    outcome = result.outcome\n",
        "    output = {'outcome':outcome}\n",
        "    if outcome=='pass':\n",
        "        details = json.loads(result.value_override)\n",
        "        output.update(details)\n",
        "        if verbose:\n",
        "            print(f'Validation Result: {outcome}, Details: {details}')\n",
        "    elif outcome=='fail':\n",
        "        details = json.loads(result.error_message)\n",
        "        output.update(details)\n",
        "        output['response'] = result.fix_value\n",
        "        if verbose:\n",
        "            print(f'Validation Result: {outcome}, Details: {details}, '\n",
        "                  f'Bot Reponse: {result.fix_value}')\n",
        "    return output\n",
        "\n",
        "# Guardrails\n",
        "@register_validator(name=\"hullucination\", data_type=\"string\")\n",
        "class HallucinationGuardrail(Validator):\n",
        "    \"\"\"\n",
        "    A custom validator that detects hallucinations in model-generated text by\n",
        "    comparing it with a reference premise using a natural language inference\n",
        "    (NLI) model. The validator uses the model's output to assess whether the\n",
        "    generated text is entailed by, neutral to, or contradicts the premise.\n",
        "\n",
        "    - If the output is 'entailment' or 'neutral', it passes the validation.\n",
        "    - If it's 'contradiction' but the confidence score is below a threshold,\n",
        "      it is considered neutral and passes.\n",
        "    - Otherwise, the validation fails with an error message.\n",
        "\n",
        "    Attributes:\n",
        "        model: A callable NLI model that takes a dict with \"text\"\n",
        "               and \"text_pair\".\n",
        "        threshold (str): The maximum score at which a contradiction is treated\n",
        "                         as neutral.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, threshold:str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.threshold=threshold\n",
        "\n",
        "    def _validate(self, value: str, premise: str):\n",
        "        self.premise = premise\n",
        "        result = self.model({\"text\": self.premise, \"text_pair\": value})\n",
        "        label = result['label'].lower()\n",
        "        score = result['score']\n",
        "\n",
        "        if label==\"entailment\":\n",
        "            return PassResult(value_override=json.dumps(result))\n",
        "\n",
        "        # Note: The NLI used by this demo does not return neutral label\n",
        "        elif label==\"neutral\":\n",
        "            return PassResult(value_override=json.dumps(result))\n",
        "        elif label==\"contradiction\" and score<self.threshold:\n",
        "            return PassResult(\n",
        "                value_override=json.dumps({\"label\":\"neutral\",\"score\":score}))\n",
        "        else:\n",
        "            return FailResult(\n",
        "                error_message=json.dumps(result),\n",
        "                fix_value=\"I'm sorry, I couldn't find enough information \"\n",
        "                          \"to answer that accurately.\")\n",
        "\n",
        "@register_validator(name=\"toxicity\", data_type=\"string\")\n",
        "class ToxicityGuardrail(Validator):\n",
        "    \"\"\"\n",
        "    A custom validator to detect toxic language using a text classification\n",
        "    model. The validator assesses if the input is toxic based on the model's\n",
        "    label and score, and applies a configurable threshold to determine the\n",
        "    outcome.\n",
        "\n",
        "    - If the input is labeled as 'non-toxic', it passes.\n",
        "    - If labeled as 'toxic' and no threshold is provided, or the score exceeds\n",
        "      the threshold, it fails.\n",
        "    - If the toxicity score is below the threshold, it passes, treating the\n",
        "      message as 'non-toxic'.\n",
        "\n",
        "    Attributes:\n",
        "        model: A callable classification model returning a list with 'label'\n",
        "               and 'score'.\n",
        "        threshold (float, optional): A score threshold below which 'toxic'\n",
        "                                     inputs are treated as non-toxic.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, threshold:float=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def _validate(self, value: str):\n",
        "        result = self.model(value)[0]\n",
        "        label = result['label']\n",
        "        score = result['score']\n",
        "        if label=='non-toxic':\n",
        "            return PassResult(value_override=json.dumps(result))\n",
        "\n",
        "        elif (label=='toxic' and self.threshold is None) or (\n",
        "              label=='toxic' and self.threshold < score):\n",
        "            return FailResult(\n",
        "                error_message=json.dumps(result),\n",
        "                fix_value=\"I can't continue the conversation if inappropriate \"\n",
        "                          \"language is used. Let me know how I can help \"\n",
        "                          \"respectfully.\")\n",
        "\n",
        "        elif label=='toxic' and score<self.threshold:\n",
        "            return PassResult(\n",
        "                value_override=json.dumps({'label':'non-toxic', 'score':score}))\n",
        "\n",
        "@register_validator(name=\"english_input\", data_type=\"string\")\n",
        "class EnglishInputGuardrail(Validator):\n",
        "    \"\"\"\n",
        "    A custom validator to check if the input text is in English. It uses a\n",
        "    language detection model and validates based on the predicted language.\n",
        "\n",
        "    - If the input is in English ('en-US'), validation passes.\n",
        "    - If the input is in another language, validation fails.\n",
        "\n",
        "    Attributes:\n",
        "        model: A language detection model that returns 'label' and 'score'.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "\n",
        "    def _validate(self, value: str):\n",
        "        result = self.model(value)[0]\n",
        "        label = result['label']\n",
        "        score = result['score']\n",
        "        if label=='en-US':\n",
        "            return PassResult(value_override=json.dumps(result))\n",
        "        else:\n",
        "            return FailResult(\n",
        "                error_message=json.dumps(result),\n",
        "                fix_value=\"Oops! I’m still learning new languages. But for now\"\n",
        "                          \", I can only chat confidently in English.\")\n",
        "\n",
        "@register_validator(name=\"topic\", data_type=\"string\")\n",
        "class TopicGuardrail(Validator):\n",
        "    \"\"\"\n",
        "    A custom validator to check whether the input text belongs to an allowed\n",
        "    or banned topic. It uses a classification model to label the input and\n",
        "    validates based on topic lists.\n",
        "\n",
        "    - If the topic is banned, validation fails with a fixed response.\n",
        "    - If the topic is allowed, validation passes.\n",
        "    - If the topic is unknown, validation fails.\n",
        "\n",
        "    Attributes:\n",
        "        model: A classification model returning 'labels' and 'scores'.\n",
        "        allowed_topics (list): Topics that are permitted.\n",
        "        banned_topics (list): Topics that are restricted.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, allowed_topics, banned_topics, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.allowed_topics = allowed_topics\n",
        "        self.banned_topics = banned_topics\n",
        "        self.model = model\n",
        "\n",
        "    def _validate(self, value: str):\n",
        "        result = self.model(value, self.allowed_topics+self.banned_topics)\n",
        "        label = result['labels'][0]\n",
        "        top_result = {'label': label, 'score':result['scores'][0]}\n",
        "\n",
        "        if label in self.banned_topics:\n",
        "            return FailResult(\n",
        "                error_message=json.dumps(top_result),\n",
        "                fix_value=\"Let's keep it focused on credit cards. \"\n",
        "                          \"Happy to help with anything in that space!\")\n",
        "        elif label in self.allowed_topics:\n",
        "            return PassResult(value_override=json.dumps(top_result))\n",
        "        else:\n",
        "            return FailResult(\n",
        "                error_message=json.dumps(top_result),\n",
        "                fix_value=\"Let's keep it focused on credit cards. \"\n",
        "                          \"Happy to help with anything in that space!\")\n",
        "\n",
        "@register_validator(name=\"competitor\", data_type=\"string\")\n",
        "class CompetitorGuardrail(Validator):\n",
        "    \"\"\"\n",
        "    A custom validator to detect mentions of competitors in user input.\n",
        "    It uses regex pattern matching to search for competitor names and\n",
        "    brands in the text.\n",
        "\n",
        "    - If a competitor name is found, validation fails.\n",
        "    - If no match is found, validation passes.\n",
        "\n",
        "    Attributes:\n",
        "        competitors (dict): A dictionary mapping competitor names to their\n",
        "                            associated brand names.\n",
        "    \"\"\"\n",
        "    def __init__(self, competitors:Dict[str, str], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.competitors=competitors\n",
        "\n",
        "    def _validate(self, value: str):\n",
        "        pattern = re.compile(\n",
        "            r'\\b(' + '|'.join(\n",
        "                re.escape(name) for name in \\\n",
        "                    list(self.competitors.keys()) + \\\n",
        "                    list(self.competitors.values())) + r')\\b',\n",
        "            flags=re.IGNORECASE\n",
        "        )\n",
        "        matches = pattern.findall(value)\n",
        "        if matches:\n",
        "            return FailResult(\n",
        "                error_message=json.dumps({\n",
        "                    'label':matches, 'score':[1.00]*len(matches)}),\n",
        "                fix_value=\"I'm here to help with questions about our products \"\n",
        "                          \"only, so I can't comment on other institutions.\")\n",
        "        else:\n",
        "            return PassResult(\n",
        "                value_override=json.dumps({'label':['no-matches'], 'score':[1.00]}))\n",
        "\n",
        "@register_validator(name=\"PII\", data_type=\"string\")\n",
        "class PIIGuardrail(Validator):\n",
        "    \"\"\"\n",
        "    A custom validator to detect and handle personally identifiable\n",
        "    information (PII) in user input. It uses an analyzer to identify PII\n",
        "    entities and an anonymizer to optionally mask them.\n",
        "\n",
        "    - If PII is detected, validation fails and returns a privacy warning.\n",
        "    - If no PII is found, validation passes.\n",
        "\n",
        "    Attributes:\n",
        "        analyzer: A PII analyzer that identifies entity types in text.\n",
        "        anonymizer: A tool that can anonymize detected PII.\n",
        "        pii_entities (list, optional): Specific PII entities to scan for.\n",
        "    \"\"\"\n",
        "    def __init__(self, analyzer, anonymizer, pii_entities:List[str]=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.analyzer = analyzer\n",
        "        self.pii_entities=pii_entities\n",
        "        self.anonymizer = anonymizer\n",
        "\n",
        "    def _validate(self, value: str):\n",
        "        if self.pii_entities is not None:\n",
        "            analysis = self.analyzer.analyze(\n",
        "                value, entities=self.pii_entities, language='en')\n",
        "        else:\n",
        "            analysis = self.analyzer.analyze(\n",
        "                value, language='en')\n",
        "\n",
        "        if len(analysis):\n",
        "            result = {'label':[a.entity_type for a in analysis],\n",
        "                      'score':[a.score for a in analysis]}\n",
        "\n",
        "            new_value = self.anonymizer.anonymize(\n",
        "                text=value, analyzer_results=analysis)\n",
        "\n",
        "            return FailResult(\n",
        "                error_message = json.dumps(result),\n",
        "                fix_value=\"Privacy is a big deal! Let's keep personal info \"\n",
        "                          \"out of our chat. \"\n",
        "                          \"Feel free to rephrase your question. \")\n",
        "                # Decision point: Do we want to redact user input?\n",
        "                # if so, then `fix_value=new_value.text`\n",
        "        else:\n",
        "            return PassResult(\n",
        "                value_override=json.dumps({'label':['N/A'], 'score':[0]}))"
      ],
      "metadata": {
        "id": "1tPNEfs9Ynq2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Setup RAG-Based Chatbot\n",
        "\n",
        "- Create a simple RAG chatbot to test all implemented guardrails.\n",
        "- Key Tech Stack:\n",
        "    - **Retrieval and Augmentation**:\n",
        "        - Chunking: `LangChain`\n",
        "        - Embedding: `all-MiniLM-L6-v2`\n",
        "        - Vector DB & Semantic Search: `FAISS`\n",
        "    - **Generation**:\n",
        "        - **GenAI Model**: `Gemma-7b-it`"
      ],
      "metadata": {
        "id": "B2z-7SlB7-7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "vram = round(torch.cuda.get_device_properties(0).total_memory/1024**3,2)\n",
        "print(f\"GPU vRAM: {vram} GB\")\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9cH_gZfHUfL",
        "outputId": "c315a179-1777-476e-9bca-c397339f1f14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU vRAM: 14.74 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Note: This cell will take a while to run (<=5 min in Colab Pro)\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Setup embeding model - 90MB required\n",
        "emb_model_id = \"sentence-transformers/all-minilm-l6-v2\"\n",
        "emb_model = SentenceTransformer(model_name_or_path=emb_model_id).to(device)\n",
        "\n",
        "# Setup LLM - 17GB disk space required\n",
        "llm_id = \"google/gemma-7b-it\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=llm_id)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=llm_id,\n",
        "    torch_dtype=torch.float32,\n",
        "    quantization_config=quantization_config,\n",
        "    low_cpu_mem_usage=True).to(device)"
      ],
      "metadata": {
        "id": "DwIgAVYO9rOB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Chunking: PDF to Chunks"
      ],
      "metadata": {
        "id": "sYJkJzW8f0KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = \"\\n\".join([page.get_text() for page in doc])\n",
        "texts = texts.replace('\\n', ' ').replace(\"•\", ' ').strip()\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "chunks = splitter.split_text(texts)"
      ],
      "metadata": {
        "id": "X2ECVu3G9fOp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Embedding: Chunks to Vector DB"
      ],
      "metadata": {
        "id": "fxoapzaJtkHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = emb_model.encode(chunks, convert_to_numpy=True)\n",
        "vector_db = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "vector_db.add(embeddings)"
      ],
      "metadata": {
        "id": "ytcLVKi5v-hl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Context Retrival: Sementic/Vector Search"
      ],
      "metadata": {
        "id": "hT5wWXi-Azpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context_from_db(query:str,\n",
        "                             vector_db,\n",
        "                             emb_model,\n",
        "                             num_chunks_to_retrieve:int=3,\n",
        "                             verbose:bool=True) -> tuple:\n",
        "    \"\"\"\n",
        "    Retrieve context chunks from vector database\n",
        "    Colin Li @ 2025-03\n",
        "    \"\"\"\n",
        "    # Embed query and generate top matches based on distance\n",
        "    query_embedding = emb_model.encode([query])\n",
        "    dist, idx = vector_db.search(np.array(query_embedding),\n",
        "                             k=num_chunks_to_retrieve)\n",
        "    # Retrieve context based on distance\n",
        "    ret_scores = dist[0].tolist()\n",
        "    ret_texts = [chunks[i] for i in idx[0]]\n",
        "    return ret_scores, ret_texts"
      ],
      "metadata": {
        "id": "ziYLE9NeMRtn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_augmented_prompt(query:str,\n",
        "                           base_prompt:str,\n",
        "                           ret_texts:list, verbose=False):\n",
        "    \"\"\"\n",
        "    Build augmented prompt for the RAG pipeline\n",
        "    Colin Li @ 2025-03\n",
        "    \"\"\"\n",
        "    # Create augmented prompt\n",
        "    context = \"\\n\".join([f\"Context {i+1}: {t}\" for i, t in enumerate(ret_texts)])\n",
        "    augmented_prompt = \\\n",
        "    f\"\"\"\\\n",
        "    {base_prompt}:\n",
        "    {context}\n",
        "    User query: {query}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # Dedent\n",
        "    augmented_prompt = augmented_prompt.replace(\"    \", \"\")\n",
        "\n",
        "    # Apply template\n",
        "    dialogue_template = [{\"role\": \"user\", \"content\": augmented_prompt}]\n",
        "    if verbose:\n",
        "        print(augmented_prompt)\n",
        "\n",
        "    return dialogue_template, context"
      ],
      "metadata": {
        "id": "9MSPl6JzNxmr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Augmented Generation Using Open Source GenAI"
      ],
      "metadata": {
        "id": "bM973ciLdQPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query:str,\n",
        "                      base_prompt:str,\n",
        "                      vector_db,\n",
        "                      tokenizer,\n",
        "                      llm,\n",
        "                      device:str,\n",
        "                      max_new_tokens:int=150,\n",
        "                      temperature:int=0.9,\n",
        "                      verbose=True):\n",
        "\n",
        "    ret_scores, ret_texts = \\\n",
        "    retrieve_context_from_db(query, vector_db, emb_model, verbose=False)\n",
        "    dialogue_template, premise = build_augmented_prompt(\n",
        "        query, base_prompt, ret_texts, verbose=False)\n",
        "\n",
        "    # Apply the chat template\n",
        "    augmented_prompt = tokenizer.apply_chat_template(\n",
        "        conversation=dialogue_template,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True)\n",
        "    if verbose:\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Retrieved Text: {ret_texts}\")\n",
        "        print(f\"Dialogue Template: {dialogue_template}\")\n",
        "        print(f\"Augmented Prompt in Template:\\n{augmented_prompt}\")\n",
        "\n",
        "    # Tokenise prompt\n",
        "    ids_in = tokenizer(augmented_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output\n",
        "    ids_out = llm.generate(**ids_in,\n",
        "                           max_new_tokens=max_new_tokens,\n",
        "                           temperature=temperature,\n",
        "                           do_sample=True)\n",
        "    text_out = tokenizer.decode(ids_out[0])\n",
        "    text_out = text_out\\\n",
        "        .replace(augmented_prompt, '')\\\n",
        "        .replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\\\n",
        "        .replace(\"Sure, here is the answer to the user query based on the provided context:\\n\\n\", \"\")\n",
        "\n",
        "    return text_out, premise"
      ],
      "metadata": {
        "id": "y2k-MY_EsF-Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Test RAG"
      ],
      "metadata": {
        "id": "n2-DrZgl3N3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_prompt = (\n",
        "    \"Use the following retrieved context to answer the user's query concisely \"\n",
        "    \"and accurately. Prioritise relevance and avoid adding any information \"\n",
        "    \"that isn't in the provided context. Include all necessary details to give \"\n",
        "    \"a clear and complete answer. Do not mention or refer to the context \"\n",
        "    \"source. Only respond to what's asked in the user query. \"\n",
        "    \"Think carefully before answering.\"\n",
        ")\n",
        "query = 'do i pay additional cardholder fee?'\n",
        "text_out, premise = generate_response(\n",
        "    query=query,\n",
        "    base_prompt=base_prompt,\n",
        "    vector_db=vector_db,\n",
        "    tokenizer=tokenizer,\n",
        "    llm=llm,\n",
        "    device=device,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.5,\n",
        "    verbose=False)\n",
        "print(premise)\n",
        "print(text_out)"
      ],
      "metadata": {
        "id": "ikTZFLPv0ou_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecb2e15-c368-4093-a1f4-407f033d7c9c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1: & fees  Conditions  Additional Cardholder Fee  Free  Pay no additional cardholder fee to share the\n",
            "Context 2: fee to share the convenience of your card with  someone else.  Cash Advance Rate  21.99% p.a.  Cash\n",
            "Context 3: worldwide and up-to-date travel information7     No additional cardholder fee to share your card\n",
            "Based on the provided context, the answer to the user query is:\n",
            "\n",
            "No additional cardholder fee is required to share your card in both contexts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Implementation of Guardrails"
      ],
      "metadata": {
        "id": "JRuhYh5DC1rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Implementation Logic"
      ],
      "metadata": {
        "id": "01m6TP3tGBhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_input_gr(hypothesis, premise=None, gr_db:Dict=None):\n",
        "    \"\"\"\n",
        "    Logic to implement input guardrails\n",
        "    \"\"\"\n",
        "\n",
        "    # English Input\n",
        "    gr_1 = EnglishInputGuardrail(model=lan_class_model)\n",
        "    gr_1_eval = get_result(gr_1._validate(hypothesis), verbose=False)\n",
        "\n",
        "    # OOS Topics:\n",
        "    gr_2 = TopicGuardrail(model=topic_model, allowed_topics=allowed_topics,\n",
        "                          banned_topics=banned_topics)\n",
        "    gr_2_eval = get_result(gr_2._validate(hypothesis), verbose=False)\n",
        "\n",
        "    # Competitors (REGEX):\n",
        "    gr_3 = CompetitorGuardrail(competitors=competitors_dict)\n",
        "    gr_3_eval = get_result(gr_3._validate(hypothesis), verbose=False)\n",
        "\n",
        "    # PII:\n",
        "    gr_4 = PIIGuardrail(analyzer=analyzer, anonymizer=anonymizer,\n",
        "                        pii_entities=pii_entities)\n",
        "    gr_4_eval = get_result(gr_4._validate(hypothesis), verbose=False)\n",
        "\n",
        "    # Log all guardrail results\n",
        "    if gr_db is not None:\n",
        "        gr_db[datetime.today()] = gr_1_eval\n",
        "        gr_db[datetime.today()] = gr_2_eval\n",
        "        gr_db[datetime.today()] = gr_3_eval\n",
        "        gr_db[datetime.today()] = gr_4_eval\n",
        "\n",
        "    # The standardised response from the first failed guardrail result\n",
        "    results = (gr_1_eval, gr_2_eval, gr_3_eval, gr_4_eval)\n",
        "    for result in results:\n",
        "        if result.get(\"outcome\") == \"fail\":\n",
        "            return {\n",
        "                'use_rag':False,\n",
        "                'response': result[\"response\"],\n",
        "                'context': premise,\n",
        "                'gr_english':gr_1_eval,\n",
        "                'gr_topics': gr_2_eval,\n",
        "                'gr_competitors': gr_3_eval,\n",
        "                'gr_pii': gr_4_eval}\n",
        "\n",
        "    # The LLM response from last guardrail output if all guardrails are pass\n",
        "    else:\n",
        "        return {\n",
        "            'use_rag': True,\n",
        "            'response': None, # No response when all gr passed\n",
        "            'context': premise,\n",
        "            'gr_english': gr_1_eval,\n",
        "            'gr_topics': gr_2_eval,\n",
        "            'gr_competitors': gr_3_eval,\n",
        "            'gr_pii': gr_4_eval}"
      ],
      "metadata": {
        "id": "LwDzmArL4gVB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_output_gr(hypothesis, premise, gr_db:Dict=None):\n",
        "    \"\"\"\n",
        "    Logic to implement output guardrails\n",
        "    \"\"\"\n",
        "\n",
        "    # Hallucination\n",
        "    gr_1 = HallucinationGuardrail(model=nli_model_1, threshold=.9718)\n",
        "    gr_1_eval = get_result(gr_1._validate(hypothesis, premise), verbose=False)\n",
        "    gr_1_eval['response_0'] = hypothesis\n",
        "\n",
        "    # Toxicity\n",
        "    gr_2 = ToxicityGuardrail(model=toxicity_model,threshold=None)\n",
        "    gr_2_eval = get_result(gr_2._validate(hypothesis), verbose=False)\n",
        "    gr_2_eval['response_0'] = hypothesis\n",
        "\n",
        "    # Log all guardrail results\n",
        "    if gr_db is not None:\n",
        "        gr_db[datetime.today()] = gr_1_eval\n",
        "        gr_db[datetime.today()] = gr_2_eval\n",
        "\n",
        "    # Key Logic: The standardised response from the first failed guardrail\n",
        "    results = (gr_1_eval, gr_2_eval)\n",
        "    for result in results:\n",
        "        if result.get(\"outcome\") == \"fail\":\n",
        "            return {\n",
        "                'use_rag': True,\n",
        "                'response': result[\"response\"],\n",
        "                'context': premise,\n",
        "                'gr_hallucination': gr_1_eval,\n",
        "                'gr_toxicity': gr_2_eval}\n",
        "\n",
        "    # The LLM response from last guardrail output if all guardrails are pass\n",
        "    # Note: This allows the data to be pass to UI\n",
        "    else:\n",
        "        return {\n",
        "            'use_rag': True,\n",
        "            'response': results[-1][\"response_0\"],\n",
        "            'context': premise,\n",
        "            'gr_hallucination': gr_1_eval,\n",
        "            'gr_toxicity': gr_2_eval}"
      ],
      "metadata": {
        "id": "0C6WF4gj1sJm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_gradio(gr_ip, gr_op=None):\n",
        "    \"\"\"\n",
        "    Prepare data for gradio UI\n",
        "    \"\"\"\n",
        "\n",
        "    # Extracting values from gr_ip dictionary\n",
        "    # English Guardrail\n",
        "    in_eng_outcome = gr_ip['gr_english']['outcome']\n",
        "    in_eng_label = gr_ip['gr_english']['label']\n",
        "    in_eng_score = gr_ip['gr_english']['score']\n",
        "\n",
        "    # Topics Guardrail\n",
        "    in_topic_outcome = gr_ip['gr_topics']['outcome']\n",
        "    in_topic_label = gr_ip['gr_topics']['label']\n",
        "    in_topic_score = gr_ip['gr_topics']['score']\n",
        "\n",
        "    # PII Guardrail\n",
        "    in_pii_outcome = gr_ip['gr_pii']['outcome']\n",
        "    in_pii_label = ', '.join(map(str, gr_ip['gr_pii']['label']))\n",
        "    in_pii_score = ', '.join(map(str, gr_ip['gr_pii']['score']))\n",
        "\n",
        "    # Competitors Guardrail\n",
        "    in_comp_outcome = gr_ip['gr_competitors']['outcome']\n",
        "    in_comp_label = ', '.join(map(str, gr_ip['gr_competitors']['label']))\n",
        "    in_comp_score = ', '.join(map(str, gr_ip['gr_competitors']['score']))\n",
        "\n",
        "    if gr_ip['use_rag']:\n",
        "        response_box = gr_op['response']\n",
        "        context_box = gr_op['context']\n",
        "\n",
        "        # Hallucination Guardrail\n",
        "        out_hallu_outcome = gr_op['gr_hallucination']['outcome']\n",
        "        out_hallu_label = gr_op['gr_hallucination']['label']\n",
        "        out_hallu_score = gr_op['gr_hallucination']['score']\n",
        "        out_hallu_response = gr_op['gr_hallucination']['response_0']\n",
        "\n",
        "        # Toxicity Guardrail\n",
        "        out_tox_outcome = gr_op['gr_toxicity']['outcome']\n",
        "        out_tox_label = gr_op['gr_toxicity']['label']\n",
        "        out_tox_score = gr_op['gr_toxicity']['score']\n",
        "        out_tox_response = gr_op['gr_toxicity']['response_0']\n",
        "\n",
        "    else:\n",
        "        response_box = gr_ip['response']\n",
        "        context_box = gr_ip['context']\n",
        "\n",
        "        # Hallucination Guardrail\n",
        "        out_hallu_outcome = None\n",
        "        out_hallu_label = None\n",
        "        out_hallu_score = None\n",
        "        out_hallu_response = None\n",
        "\n",
        "        # Toxicity Guardrail\n",
        "        out_tox_outcome = None\n",
        "        out_tox_label = None\n",
        "        out_tox_score = None\n",
        "        out_tox_response = None\n",
        "\n",
        "    return (context_box, response_box, in_eng_outcome, in_eng_label, in_eng_score,\n",
        "            in_topic_outcome, in_topic_label, in_topic_score, in_pii_outcome,\n",
        "            in_pii_label, in_pii_score, in_comp_outcome, in_comp_label,\n",
        "            in_comp_score, out_hallu_outcome, out_hallu_label, out_hallu_score,\n",
        "            out_hallu_response, out_tox_outcome, out_tox_label, out_tox_score,\n",
        "            out_tox_response)"
      ],
      "metadata": {
        "id": "ITHcf5ZiRp2b"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Demo - Chatbot Response without Guardrails"
      ],
      "metadata": {
        "id": "-1frDGRhmoze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "def chatbot_no_gr(query):\n",
        "    text_out, premise = generate_response(\n",
        "        query=query,\n",
        "        base_prompt=base_prompt,\n",
        "        vector_db=vector_db,\n",
        "        tokenizer=tokenizer,\n",
        "        llm=llm,\n",
        "        device=device,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.5,\n",
        "        verbose=False)\n",
        "    return text_out"
      ],
      "metadata": {
        "id": "nb8ZpdU_vvwm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo\n",
        "print(chatbot_no_gr(\"Shitty service!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG8JyyWHv50n",
        "outputId": "abc7467e-28f2-4b6d-c48d-017e71fde4f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text does not contain any information about the user's query, therefore I cannot provide an answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Demo - Chatbot Response with Guardrails"
      ],
      "metadata": {
        "id": "EwhU7u7em0xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "gr_db = dict()\n",
        "def chatbot(query):\n",
        "    gr_ip = implement_input_gr(hypothesis=query, premise=None, gr_db=None)\n",
        "    if gr_ip['use_rag']:\n",
        "        text_out, premise = generate_response(\n",
        "            query=query,\n",
        "            base_prompt=base_prompt,\n",
        "            vector_db=vector_db,\n",
        "            tokenizer=tokenizer,\n",
        "            llm=llm,\n",
        "            device=device,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.5,\n",
        "            verbose=False)\n",
        "        gr_op = implement_output_gr(hypothesis=text_out,\n",
        "                                    premise=premise, gr_db=None)\n",
        "        return prepare_data_for_gradio(gr_ip=gr_ip, gr_op=gr_op)\n",
        "    else:\n",
        "        return prepare_data_for_gradio(gr_ip=gr_ip, gr_op=None)"
      ],
      "metadata": {
        "id": "w7HPBUSKRE8e"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo\n",
        "print(chatbot('do i pay for international transaction fee?')[1])\n",
        "print(chatbot('Shitty service!')[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK65QKdECIXN",
        "outputId": "036e0b16-5590-4323-a9f7-a2502ff1dc5c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the answer to the user's query is:\n",
            "\n",
            "No international transaction fees are payable for purchases made overseas or online.\n",
            "Let's keep it focused on credit cards. Happy to help with anything in that space!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 See the Guardrails in Action: UI and Demo\n",
        "- Click on Gradio URL, so you can open it in your broswer as a new tab"
      ],
      "metadata": {
        "id": "4kh1y9r0qPwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UI logic\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # I/O\n",
        "    gr.Markdown(\"## 💬 Credit Card Chatbot with Guardrail Outputs\")\n",
        "    user_input = gr.Textbox(label=\"Ask a question\",\n",
        "                            placeholder=\"e.g. How do I earn bonus points?\")\n",
        "    send_btn = gr.Button(\"Submit\")\n",
        "\n",
        "    gr.Markdown(\"**I/O**\")\n",
        "    with gr.Row():\n",
        "        context_box = gr.Textbox(label=\"📘 Retrieved Context\", lines=3)\n",
        "        response_box = gr.Textbox(label=\"🤖 Bot Response (with Guardrail)\",\n",
        "                                  lines=3)\n",
        "\n",
        "    # Guardrail\n",
        "    gr.Markdown(\"### Guardrail Results\")\n",
        "    with gr.Row():\n",
        "\n",
        "        # LEFT: Input Guardrails\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"#### 🔍 Input Guardrails\")\n",
        "            with gr.Row():\n",
        "                in_eng_outcome = gr.Textbox(label=\"English Check - Outcome\")\n",
        "                in_eng_label = gr.Textbox(\n",
        "                    label=\"English Check - Detected Language\")\n",
        "                in_eng_score = gr.Textbox(label=\"English Check - Confidence\")\n",
        "            with gr.Row():\n",
        "                in_topic_outcome = gr.Textbox(label=\"Topic Check - Outcome\")\n",
        "                in_topic_label = gr.Textbox(label=\"Topic Check - Label\")\n",
        "                in_topic_score = gr.Textbox(label=\"Topic Check - Score\")\n",
        "            with gr.Row():\n",
        "                in_comp_outcome = gr.Textbox(label=\"Competitor Check - Outcome\")\n",
        "                in_comp_label = gr.Textbox(label=\"Competitor Check - Match\")\n",
        "                in_comp_score = gr.Textbox(label=\"Competitor Check - Score\")\n",
        "            with gr.Row():\n",
        "                in_pii_outcome = gr.Textbox(label=\"PII Check - Outcome\")\n",
        "                in_pii_label = gr.Textbox(label=\"PII Check - Entity Types\")\n",
        "                in_pii_score = gr.Textbox(label=\"PII Check - Scores\")\n",
        "\n",
        "        # RIGHT: Output Guardrails\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"#### 🧠 Output Guardrails\")\n",
        "            with gr.Row():\n",
        "                out_hallu_outcome = gr.Textbox(\n",
        "                    label=\"Hallucination - Outcome\", scale=1)\n",
        "                out_hallu_label = gr.Textbox(\n",
        "                    label=\"Hallucination - NLI Label\", scale=1)\n",
        "                out_hallu_score = gr.Textbox(\n",
        "                    label=\"Hallucination - NLI Score\", scale=1)\n",
        "            with gr.Row():\n",
        "                out_hallu_response = gr.Textbox(\n",
        "                    label=\"Bot Response (Pre-Hallucination Filter)\",\n",
        "                    scale=3, lines=2)\n",
        "            with gr.Row():\n",
        "                out_tox_outcome = gr.Textbox(label=\"Toxicity - Outcome\")\n",
        "                out_tox_label = gr.Textbox(label=\"Toxicity - Label\")\n",
        "                out_tox_score = gr.Textbox(label=\"Toxicity - Score\")\n",
        "            with gr.Row():\n",
        "                out_tox_response = gr.Textbox(\n",
        "                    label=\"Bot Response (Pre-Toxicity Filter)\", lines=2)\n",
        "\n",
        "    # Trigger chatbot function when button is clicked\n",
        "    send_btn.click(\n",
        "        fn=chatbot,\n",
        "        inputs=[user_input],\n",
        "\n",
        "        # Output from the function will be used to populate data to boxes in UI\n",
        "        outputs=[\n",
        "            # Shared\n",
        "            context_box, response_box,\n",
        "\n",
        "            # Input GR\n",
        "            in_eng_outcome, in_eng_label, in_eng_score,\n",
        "            in_topic_outcome, in_topic_label, in_topic_score,\n",
        "            in_pii_outcome, in_pii_label, in_pii_score,\n",
        "            in_comp_outcome, in_comp_label, in_comp_score,\n",
        "            # Output GR\n",
        "            out_hallu_outcome, out_hallu_label, out_hallu_score, out_hallu_response,\n",
        "            out_tox_outcome, out_tox_label, out_tox_score, out_tox_response\n",
        "        ]\n",
        "    )\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "fK5FJSKcackm",
        "outputId": "9e70c948-0076-455d-f865-507ba3a3ba67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b2a726333bb237721c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b2a726333bb237721c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix: Generate Groundedness Check for Human Review\n",
        "- Responses were generated for below questions.\n",
        "- Groundedness was checked."
      ],
      "metadata": {
        "id": "nAja3o7JzF9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "on_topic_questions = [\n",
        "    \"What is the Ultimate Awards credit card?\",\n",
        "    \"How can I earn 100,000 bonus Awards points?\",\n",
        "    \"What’s the minimum spend to qualify for the bonus offer?\",\n",
        "    \"What purchases are excluded from the bonus point calculation?\",\n",
        "    \"How do I opt-in to earn Qantas Points instead of Awards points?\",\n",
        "    \"How much does it cost to opt-in for Qantas Points?\",\n",
        "    \"What’s the conversion rate from Awards points to Qantas Points?\",\n",
        "    \"Are there any international transaction fees?\",\n",
        "    \"What’s the interest rate on purchases?\",\n",
        "    \"How do I activate international travel insurance?\",\n",
        "    \"What is the monthly fee and how can I avoid it?\",\n",
        "    \"What happens if I spend more than $10,000 in a statement period?\",\n",
        "    \"How do I redeem my Awards points?\",\n",
        "    \"Can I use points to book flights or hotels?\",\n",
        "    \"Do the Awards points expire?\",\n",
        "    \"What is CommBank Yello Cashback Offers?\",\n",
        "    \"How do I add an additional cardholder?\",\n",
        "    \"Can I use my card overseas?\",\n",
        "    \"What are the airport lounge benefits with this card?\",\n",
        "    \"Where can I check my Awards points balance?\",\n",
        "]\n",
        "\n",
        "off_topic_questions = [\n",
        "    \"Who has the best credit card in Australia?\",\n",
        "    \"Can you tell me a joke about banks?\",\n",
        "    \"Do you think this card is better than ANZ’s?\",\n",
        "    \"Should I invest in Bitcoin instead?\",\n",
        "    \"What’s your opinion on government spending?\",\n",
        "    \"Can you write a rap about Qantas Points?\",\n",
        "    \"Are interest rates going to rise this year?\",\n",
        "    \"I hate all banks — what should I do?\",\n",
        "    \"Can I use this card to buy a tank?\",\n",
        "    \"Which politician gives better financial advice?\"\n",
        "]"
      ],
      "metadata": {
        "id": "d8K9nkRWzHJB"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}